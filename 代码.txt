#Load the required toolkit
import os
import numpy as np
import cv2
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import cohen_kappa_score

import paddle
import paddle.nn as nn
import paddle.nn.functional as F
from paddle.nn import Conv2D, MaxPool2D, AdaptiveAvgPool2D, Linear, ReLU, BatchNorm2D
import paddle.nn.functional as F

import transforms as trans

import warnings
warnings.filterwarnings('ignore')

#Define parameters
# Config
batchsize = 16 # 4 patients per iter, i.e, 20 steps / epoch
oct_img_size = [512, 512]
image_size = 256
iters = 200 # For demonstration purposes only, far from reaching convergence
val_ratio = 0.2 # 80 / 20
trainset_root = "Glaucoma_grading/training/multi-modality_images"
gt_file = 'Glaucoma_grading/training/glaucoma_grading_training_GT.xlsx'
testset_root = "Glaucoma_grading/testing/multi-modality_images"
num_workers = 4
init_lr = 1e-4
optimizer_type = "adam"

#Unzip the dataset
!tar -xf /home/aistudio/data/data128738/gamma_grading.tar

#Training/validation data partitioning
# Train / Val splitition
filelists = os.listdir(trainset_root)
train_filelists, val_filelists = train_test_split(filelists, test_size=val_ratio, random_state=42)
print("Total Nums: {}, train: {}, val: {}".format(len(filelists), len(train_filelists), len(val_filelists)))

#Data loading
# DataLoader
# GAMMA_sub1_dataset:
#
#     Load oct images and fundus images according to `patient id`
#
class GAMMA_sub1_dataset(paddle.io.Dataset):
    """
    getitem() output:

    	fundus_img: RGB uint8 image with shape (3, image_size, image_size)

        oct_img:    Uint8 image with shape (256, oct_img_size[0], oct_img_size[1])
    """

    def __init__(self,
                 img_transforms,
                 oct_transforms,
                 dataset_root,
                 label_file='',
                 filelists=None,
                 num_classes=3,
                 mode='train'):

        self.dataset_root = dataset_root
        self.img_transforms = img_transforms
        self.oct_transforms = oct_transforms
        self.mode = mode.lower()
        self.num_classes = num_classes

        if self.mode == 'train':
            label = {row['data']: row[1:].values
                     for _, row in pd.read_excel(label_file).iterrows()}

            self.file_list = [[f, label[int(f)]] for f in os.listdir(dataset_root) ]
        elif self.mode == "test":
            self.file_list = [[f, None] for f in os.listdir(dataset_root)]

        if filelists is not None:
            self.file_list = [item for item in self.file_list if item[0] in filelists]

    def __getitem__(self, idx):
        real_index, label = self.file_list[idx]

        fundus_img_path = os.path.join(self.dataset_root, real_index, real_index + ".jpg")
        oct_series_list = sorted(os.listdir(os.path.join(self.dataset_root, real_index, real_index)),
                                 key=lambda x: int(x.strip("_")[0]))

        fundus_img = cv2.imread(fundus_img_path)[:, :, ::-1]  # BGR -> RGB
        oct_series_0 = cv2.imread(os.path.join(self.dataset_root, real_index, real_index, oct_series_list[0]),
                                  cv2.IMREAD_GRAYSCALE)
        oct_img = np.zeros((len(oct_series_list), oct_series_0.shape[0], oct_series_0.shape[1], 1), dtype="uint8")

        for k, p in enumerate(oct_series_list):
            oct_img[k] = cv2.imread(
                os.path.join(self.dataset_root, real_index, real_index, p), cv2.IMREAD_GRAYSCALE)[..., np.newaxis]

        if self.img_transforms is not None:
            fundus_img = self.img_transforms(fundus_img)
        if self.oct_transforms is not None:
            oct_img = self.oct_transforms(oct_img)

        # normlize on GPU to save CPU Memory and IO consuming.
        # fundus_img = (fundus_img / 255.).astype("float32")
        # oct_img = (oct_img / 255.).astype("float32")

        fundus_img = fundus_img.transpose(2, 0, 1)  # H, W, C -> C, H, W
        oct_img = oct_img.squeeze(-1)  # D, H, W, 1 -> D, H, W

        if self.mode == 'test':
            return fundus_img, oct_img, real_index
        if self.mode == "train":
            label = label.argmax()
            return fundus_img, oct_img, label

    def __len__(self):
        return len(self.file_list)

#Preprocess the data
img_train_transforms = trans.Compose([
    trans.RandomResizedCrop(
        image_size, scale=(0.90, 1.1), ratio=(0.90, 1.1)),
    trans.RandomHorizontalFlip(),
    trans.RandomVerticalFlip(),
    trans.RandomRotation(30)
])

oct_train_transforms = trans.Compose([
    trans.CenterCrop([256] + oct_img_size),
    trans.RandomHorizontalFlip(),
    trans.RandomVerticalFlip()
])

img_val_transforms = trans.Compose([
    trans.CropCenterSquare(),
    trans.Resize((image_size, image_size))
])

oct_val_transforms = trans.Compose([
    trans.CenterCrop([256] + oct_img_size)
])

#Load image
_train = GAMMA_sub1_dataset(dataset_root=trainset_root,
                        img_transforms=img_train_transforms,
                        oct_transforms=oct_train_transforms,
                        label_file=gt_file)

_val = GAMMA_sub1_dataset(dataset_root=trainset_root,
                        img_transforms=img_val_transforms,
                        oct_transforms=oct_val_transforms,
                        label_file=gt_file)

#display image
for i in range(5):
    fundus_img, oct_img, lab = _train.__getitem__(i)
    plt.subplot(2, 5, i+1)
    plt.imshow(fundus_img.transpose(1, 2, 0))
    plt.axis("off")
    plt.subplot(2, 5, i+6)
    plt.imshow(oct_img[100], cmap='gray')
    print(fundus_img.shape)
    plt.axis("off")
plt.figure(figsize=(15, 5))

for i in range(5):
    fundus_img, oct_img, lab = _val.__getitem__(i)
    plt.subplot(2, 5, i+1)
    plt.imshow(fundus_img.transpose(1, 2, 0))
    plt.axis("off")
    plt.subplot(2, 5, i+6)
    plt.imshow(oct_img[100], cmap='gray')
    print(oct_img[100].shape)
    plt.axis("off")

#EfficientNet
class ConvBNLayer(nn.Layer):
    def __init__(self, inchannels, outchannels, stride, kernelsize = 3, groups = 1, padding="SAME"):
        super(ConvBNLayer, self).__init__()
        padding = (kernelsize - 1) // 2
        self.conv = nn.Conv2D(
            in_channels=inchannels,
            out_channels=outchannels,
            kernel_size=kernelsize,
            stride=stride,
            padding=padding,
            groups=groups
            )
        self.bn = nn.BatchNorm2D(outchannels)
        self.Swish = nn.Swish()

    def forward(self, inputs):
        x = self.conv(inputs)
        x = self.bn(x)
        x = self.Swish(x)
        return x     

class SE(nn.Layer):
    def __init__(self, inchannels):
        super(SE, self).__init__()
        self.pooling = nn.AdaptiveAvgPool2D(output_size=(1, 1))
        self.linear0 = nn.Conv2D(in_channels=inchannels,
                        out_channels=int(inchannels*0.25), 
                        kernel_size=1)
        self.linear1 = nn.Conv2D(in_channels=int(inchannels*0.25), 
                        out_channels=inchannels, 
                        kernel_size=1)
        self.Swish = nn.Swish()
        self.Sigmoid = nn.Sigmoid()

    def forward(self, inputs):
        x = self.pooling(inputs)
        x = self.linear0(x)
        x = self.Swish(x)
        x = self.linear1(x)
        x = self.Sigmoid(x)
        out = paddle.multiply(x, inputs)
        
        return out

class MBConv(nn.Layer):
    def __init__(self, inchannels, outchannels, channels_time, kernel_size, stride, connected_dropout):
        super(MBConv, self).__init__()
        self.stride = stride
        self.layer0 = ConvBNLayer(
            inchannels=inchannels,
            outchannels=inchannels*channels_time,
            kernelsize=1,
            stride=1,
            )
        self.layer1 = ConvBNLayer(
            inchannels=inchannels*channels_time,
            outchannels=inchannels*channels_time,
            kernelsize=kernel_size,
            stride=stride,
            groups=inchannels*channels_time
            )
        self.SE = SE(inchannels = inchannels*channels_time)
        self.conv0 = nn.Conv2D(
            in_channels=inchannels*channels_time,
            out_channels=outchannels,
            kernel_size=1
            )
        self.bn0 = nn.BatchNorm2D(outchannels)
        self.conv1 = nn.Conv2D(
            in_channels=inchannels,
            out_channels=outchannels,
            kernel_size=1
            )
        self.bn1 = nn.BatchNorm2D(outchannels)
        self.dropout = nn.Dropout(p=connected_dropout)

    def forward(self, inputs):
        y = inputs
        x = self.layer0(inputs)
        x = self.layer1(x)
        x = self.SE(x)
        x = self.conv0(x)
        x = self.bn0(x)
        x = self.dropout(x)
        if self.stride == 2:
            return x          
        if self.stride == 1:
            y = self.conv1(inputs)
            y = self.bn1(y)
            return  paddle.add(x, y)

class Classifier_Head(paddle.nn.Layer):
    def __init__(self, in_channels, num_channel, dropout_rate):
        super(Classifier_Head, self).__init__()
        self.pooling = nn.AdaptiveAvgPool2D(output_size=(1, 1))
        self.conv = ConvBNLayer(inchannels=in_channels,
                                outchannels=1280,
                                kernelsize=1,
                                stride=1
                                )    
        self.dropout = nn.Dropout(p=dropout_rate) 
        self.conv1 = nn.Conv2D(
                in_channels=1280,
                out_channels=num_channel,
                kernel_size=1,
                padding="SAME"
                )
    
    def forward(self, inputs):
        x = self.conv(inputs)
        x = self.pooling(x)
        x = self.dropout(x)
        x = self.conv1(x)
        x = paddle.squeeze(x, axis=[2, 3])
        x = F.softmax(x)

        return x

class EfficientNet(nn.Layer):
    def __init__(self, in_channels, num_class, width_coefficient, depth_coefficient, connected_dropout, dropout_rate):
        super(EfficientNet, self).__init__()
        block_setting=[[0, 1, 3, 1, 1, 16],
                        [1, 2, 3, 2, 6, 24],
                        [2, 2, 5, 2, 6, 40],
                        [3, 3, 3, 2, 6, 80],
                        [4, 3, 5, 1, 6, 112],
                        [5, 4, 5, 2, 6, 192],
                        [6, 1, 3, 1, 6, 320]]
        self.block = []
        self.block.append(self.add_sublayer('c'+str(-1), nn.Conv2D(in_channels=in_channels,
                                                out_channels=int(32*width_coefficient),
                                                kernel_size=3,
                                                padding="SAME",
                                                stride=2)))
        self.block.append(self.add_sublayer('bn'+str(-1),nn.BatchNorm2D(int(32*width_coefficient))))
        i = int(32*width_coefficient)
        for j in range(int(depth_coefficient)-1):
            if  j==int(depth_coefficient)-2:
                self.block.append(self.add_sublayer('c'+str(j), nn.Conv2D(in_channels=i,
                                                out_channels=int(32*width_coefficient),
                                                kernel_size=3,
                                                padding="SAME",
                                                stride=2)))
                self.block.append(self.add_sublayer('bn'+str(j),nn.BatchNorm2D(int(32*width_coefficient))))
            else:
                self.block.append(self.add_sublayer('c'+str(j), nn.Conv2D(in_channels=i,
                                                out_channels=int(32*width_coefficient),
                                                kernel_size=3,
                                                padding="SAME")))
                self.block.append(self.add_sublayer('bn'+str(j),nn.BatchNorm2D(int(32*width_coefficient))))
        for n, r, k, s, e, o in block_setting:
            for j in range(int(r*depth_coefficient)):
                if j==int(r*depth_coefficient)-1:
                    self.block.append(self.add_sublayer('b'+str(n)+str(j), 
                                            MBConv(inchannels=i,
                                                outchannels=int(o*width_coefficient),
                                                channels_time=e,
                                                kernel_size=k,
                                                stride=s,
                                                connected_dropout=connected_dropout
                                                )))
                else:
                    self.block.append(self.add_sublayer('b'+str(n)+str(j), 
                                            MBConv(inchannels=i,
                                                outchannels=int(o*width_coefficient),
                                                channels_time=e,
                                                kernel_size=k,
                                                stride=1,
                                                connected_dropout=connected_dropout
                                                )))
                i = int(o*width_coefficient)
        self.head = Classifier_Head(in_channels=i,
                                    num_channel=num_class,
                                    dropout_rate=dropout_rate)

    def forward(self, x):
        for layer in self.block:
            x = layer(x)
        x = self.head(x)
        return x

def EfficientNetB3(in_channels, num_class):
    return EfficientNet(in_channels=in_channels, 
                        num_class=num_class,
                        width_coefficient=1.2,
                        depth_coefficient=1.4,
                        connected_dropout=0.2,
                        dropout_rate=0.3)

#resnet
class Basicblock(paddle.nn.Layer):
    def __init__(self, in_channel, out_channel, stride = 1):
        super(Basicblock, self).__init__()
        self.stride = stride
        self.conv0 = Conv2D(in_channel, out_channel, 3, stride = stride, padding = 1)
        self.conv1 = Conv2D(out_channel, out_channel, 3, stride=1, padding = 1)
        self.conv2 = Conv2D(in_channel, out_channel, 1, stride = stride)
        self.bn0 = BatchNorm2D(out_channel)
        self.bn1 = BatchNorm2D(out_channel)
        self.bn2 = BatchNorm2D(out_channel)

    def forward(self, inputs):
        y = inputs
        x = self.conv0(inputs)
        x = self.bn0(x)
        x = F.relu(x)
        x = self.conv1(x)
        x = self.bn1(x)
        if self.stride == 2:
            y = self.conv2(y)
            y = self.bn2(y)
        z = F.relu(x+y)
        return z

class Bottleneckblock(paddle.nn.Layer):
    def __init__(self, inplane, in_channel, out_channel, stride = 1, start = False):
        super(Bottleneckblock, self).__init__()
        self.stride = stride
        self.start = start
        self.conv0 = Conv2D(in_channel, inplane, 1, stride = stride)
        self.conv1 = Conv2D(inplane, inplane, 3, stride=1, padding=1)
        self.conv2 = Conv2D(inplane, out_channel, 1, stride=1)
        self.conv3 = Conv2D(in_channel, out_channel, 1, stride = stride)
        self.bn0 = BatchNorm2D(inplane)
        self.bn1 = BatchNorm2D(inplane)
        self.bn2 = BatchNorm2D(out_channel)
        self.bn3 = BatchNorm2D(out_channel)

    def forward(self, inputs):
        y = inputs
        x = self.conv0(inputs)
        x = self.bn0(x)
        x = F.relu(x)
        x = self.conv1(x)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = self.bn2(x)
        if self.start:
            y = self.conv3(y)
            y = self.bn3(y)
        z = F.relu(x+y)
        return z

class Resnet(paddle.nn.Layer):
    def __init__(self, num, bottlenet, in_channels):
        super(Resnet, self).__init__()
        self.conv0 = Conv2D(in_channels, 64, 7, stride=2)
        self.bn = BatchNorm2D(64)
        self.pool1 = MaxPool2D(3, stride=2)
        if bottlenet:
            self.layer0 = self.add_bottleneck_layer(num[0], 64, start = True)
            self.layer1 = self.add_bottleneck_layer(num[1], 128)
            self.layer2 = self.add_bottleneck_layer(num[2], 256)
            self.layer3 = self.add_bottleneck_layer(num[3], 512)
        else:
            self.layer0 = self.add_basic_layer(num[0], 64, start = True)
            self.layer1 = self.add_basic_layer(num[1], 128)
            self.layer2 = self.add_basic_layer(num[2], 256)
            self.layer3 = self.add_basic_layer(num[3], 512)
        self.pool2 = AdaptiveAvgPool2D(output_size = (1, 1))

    def add_basic_layer(self, num, inplane, start = False):
        layer = []
        if start:
            layer.append(Basicblock(inplane, inplane))
        else:
            layer.append(Basicblock(inplane//2, inplane, stride = 2))
        for i in range(num-1):
            layer.append(Basicblock(inplane, inplane))
        return nn.Sequential(*layer)

    def add_bottleneck_layer(self, num, inplane, start = False):
        layer = []
        if start:
            layer.append(Bottleneckblock(inplane, inplane, inplane*4, start = True))
        else:
            layer.append(Bottleneckblock(inplane, inplane*2, inplane*4, stride = 2, start = True))
        for i in range(num-1):
            layer.append(Bottleneckblock(inplane, inplane*4, inplane*4))
        return nn.Sequential(*layer)

    def forward(self, inputs):
        x = self.conv0(inputs)
        x = self.bn(x)
        x = self.pool1(x)
        x = self.layer0(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)        
        x = self.pool2(x)
        x = paddle.squeeze(x, axis=[2, 3])
        return x

def resnet34(in_channels=3):
    return Resnet([3, 4, 6, 3], bottlenet = False, in_channels=in_channels)

# Network
class Model(nn.Layer):
    """
    simply create a 2-branch network, and concat global pooled feature vector.
    each branch = single resnet34
    """

    def __init__(self):
        super(Model, self).__init__()
        self.fundus_branch =  EfficientNetB3(in_channels=3, num_class=1000)
        self.oct_branch = resnet34(in_channels=256)
        self.decision_branch = nn.Linear(1512 , 3)  # ResNet34 use basic block, expansion = 1



    def forward(self, fundus_img, oct_img):
        b1 = self.fundus_branch(fundus_img)
        b2 = self.oct_branch(oct_img)
        logit = self.decision_branch(paddle.concat([b1, b2], 1))

        return logit

model = Model()

#Function function
# Utils
import paddle.fluid as fluid
def train(model, iters, train_dataloader, val_dataloader, optimizer, criterion, log_interval, eval_interval):

    iter = 0
    use_cuda = True
    place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()
    model.train()
    avg_loss_list = []
    avg_kappa_list = []
    best_kappa = 0.
    while iter < iters:
        for data in train_dataloader:
            iter += 1
            if iter > iters:
                break
            fundus_imgs = (data[0] / 255.).astype("float32")
            oct_imgs = (data[1] / 255.).astype("float32")
            labels = data[2].astype('int64')
            logits = model(fundus_imgs, oct_imgs)
            loss = criterion(logits, labels)
            # acc = paddle.metric.accuracy(input=logits, label=labels.reshape((-1, 1)), k=1)
            for p, l in zip(logits.numpy().argmax(1), labels.numpy()):
                avg_kappa_list.append([p, l])

            loss.backward()
            optimizer.step()

            model.clear_gradients()
            avg_loss_list.append(loss.numpy()[0])

            if iter % log_interval == 0:
                avg_loss = np.array(avg_loss_list).mean()
                avg_kappa_list = np.array(avg_kappa_list)
                avg_kappa = cohen_kappa_score(avg_kappa_list[:, 0], avg_kappa_list[:, 1], weights='quadratic')
                avg_loss_list = []
                avg_kappa_list = []
                print("[TRAIN] iter={}/{} avg_loss={:.4f} avg_kappa={:.4f}".format(iter, iters, avg_loss, avg_kappa))

            if iter % eval_interval == 0:
                avg_loss, avg_kappa = val(model, val_dataloader, criterion)
                print("[EVAL] iter={}/{} avg_loss={:.4f} kappa={:.4f}".format(iter, iters, avg_loss, avg_kappa))
                if avg_kappa >= best_kappa:
                    best_kappa = avg_kappa
                    paddle.save(model.state_dict(),
                                os.path.join("best_model_{:.4f}".format(best_kappa), 'model.pdparams'))
                model.train()


def val(model, val_dataloader, criterion):
    model.eval()
    avg_loss_list = []
    cache = []
    with paddle.no_grad():
        for data in val_dataloader:
            fundus_imgs = (data[0] / 255.).astype("float32")
            oct_imgs = (data[1] / 255.).astype("float32")
            labels = data[2].astype('int64')

            logits = model(fundus_imgs, oct_imgs)
            for p, l in zip(logits.numpy().argmax(1), labels.numpy()):
                cache.append([p, l])

            loss = criterion(logits, labels)
            # acc = paddle.metric.accuracy(input=logits, label=labels.reshape((-1, 1)), k=1)
            avg_loss_list.append(loss.numpy()[0])
    cache = np.array(cache)
    kappa = cohen_kappa_score(cache[:, 0], cache[:, 1], weights='quadratic')
    avg_loss = np.array(avg_loss_list).mean()

    return avg_loss, kappa

#training phase
img_train_transforms = trans.Compose([
    trans.RandomResizedCrop(
        image_size, scale=(0.90, 1.1), ratio=(0.90, 1.1)),
    trans.RandomHorizontalFlip(),
    trans.RandomVerticalFlip(),
    trans.RandomRotation(30)
])

oct_train_transforms = trans.Compose([
    trans.CenterCrop([256] + oct_img_size),
    trans.RandomHorizontalFlip(),
    trans.RandomVerticalFlip()
])

img_val_transforms = trans.Compose([
    trans.CropCenterSquare(),
    trans.Resize((image_size, image_size))
])

oct_val_transforms = trans.Compose([
    trans.CenterCrop([256] + oct_img_size)
])

train_dataset = GAMMA_sub1_dataset(dataset_root=trainset_root,
                        img_transforms=img_train_transforms,
                        oct_transforms=oct_train_transforms,
                        filelists=train_filelists,
                        label_file=gt_file)

val_dataset = GAMMA_sub1_dataset(dataset_root=trainset_root,
                        img_transforms=img_val_transforms,
                        oct_transforms=oct_val_transforms,
                        filelists=val_filelists,
                        label_file=gt_file)

train_loader = paddle.io.DataLoader(
    train_dataset,
    batch_sampler=paddle.io.DistributedBatchSampler(train_dataset, batch_size=batchsize, shuffle=True, drop_last=False),
    num_workers=num_workers,
    return_list=True,
    use_shared_memory=False
)

val_loader = paddle.io.DataLoader(
    val_dataset,
    batch_sampler=paddle.io.DistributedBatchSampler(val_dataset, batch_size=batchsize, shuffle=True, drop_last=False),
    num_workers=num_workers,
    return_list=True,
    use_shared_memory=False
)
if optimizer_type == "adam":
    optimizer = paddle.optimizer.Adam(init_lr, parameters=model.parameters())
criterion = nn.CrossEntropyLoss()
train(model, iters, train_loader, val_loader, optimizer, criterion, log_interval=10, eval_interval=100)

#training phase
# Inference
'''
best_model_path = "best_model_0.7353/model.pdparams"
para_state_dict = paddle.load(best_model_path)
model.set_state_dict(para_state_dict)
model.eval()
'''

'''
img_test_transforms = trans.Compose([
    trans.CropCenterSquare(),
    trans.Resize((image_size, image_size))
])

oct_test_transforms = trans.Compose([
    trans.CenterCrop([256] + oct_img_size)
])
'''

'''
test_dataset = GAMMA_sub1_dataset(dataset_root=testset_root,
                        img_transforms=img_test_transforms,
                        oct_transforms=oct_test_transforms,
                        mode='test')
'''

'''
cache = []
for fundus_img, oct_img, idx in test_dataset:
    fundus_img = fundus_img[np.newaxis, ...]
    oct_img = oct_img[np.newaxis, ...]

    fundus_img = paddle.to_tensor((fundus_img / 255.).astype("float32"))
    oct_img = paddle.to_tensor((oct_img / 255.).astype("float32"))
    logits = model(fundus_img, oct_img)

    cache.append([idx, logits.numpy().argmax()])
'''

#submission_result = pd.DataFrame(cache, columns=['data', 'dense_pred'])

'''
submission_result = pd.DataFrame(cache, columns=['data', 'dense_pred'])
submission_result['non'] = submission_result['dense_pred'].apply(lambda x: int(x == 0))
submission_result['early'] = submission_result['dense_pred'].apply(lambda x: int(x == 1))
submission_result['mid_advanced'] = submission_result['dense_pred'].apply(lambda x: int(x == 2))
submission_result[['data', 'non', 'early', 'mid_advanced']].to_csv("./submission_sub1.csv", index=False)
'''